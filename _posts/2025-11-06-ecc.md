---
tags:
 - basics
 - coding theory
title: Error-correcting codes
#date: 2020-11-05 20:45:59
#published: false
permalink: ecc
#sidebar:
#    nav: cryptomat
#article_header:
#  type: cover
#  image:
#    src: /pictures/.jpg
---

{: .info}
**tl;dr:** Too many FRI[^BBHR18FRI] conjectures that need to be understood, so here we are...

<!--more-->

<!-- Here you can define LaTeX macros -->
<div style="display: none;">$
\def\a{\vec{a}}
\def\b{\vec{b}}
\def\L{\mathcal{L}}
\def\m{\vec{m}}
%
\def\wt#1{\mathsf{wt}\left(#1\right)}
\def\dist#1{\Delta\left(#1\right)}
\def\mindist#1{\dist{#1}}
\def\reldist#1{\delta\left(#1\right)}
\def\relmindist#1{\reldist{#1}}
\def\RS{\mathsf{RS}}
\def\CRS{\mathsf{CRS}}
\def\alphabet{\Sigma}
$</div> <!-- $ -->

## Notation

 - We use $\a \bydef (a_0, \ldots a_n)$ to denote vectors, typically _codewords_, as you'll learn later.

## Codes

An **error-correcting code** (a.k.a. a **code**) is a function that converts a **message** into a **codeword** with enough "redudancy" in it such that the original message can be recovered from a potentially-corrupted codeword.

Formally, we will restrict ourselves to **linear (block) codes** of the form:
\begin{align}
C : \alphabet^k \rightarrow \alphabet^n
\end{align}
Here, $\alphabet$ denotes the **alphabet** made up of **symbols**.

So, a message is made up of $k$ symbols while a codeword is made up of $n \ge k$ symbols:

 - $k$ is called the **dimension**
 - $n$ is called the **block length**

Note that we restrict ourselves to codes where messages and codewords share the same alphabet of symbols because this makes it natural to define a notion of [rate](#rate).

{: .smallnote}
We interchangeably use "code" (and $C$) to refer to either the encoding function above and its image $\text{Im}(C) \bydef \\{C(m)\ \vert\ m \in \Sigma^k\\}$.

As an example of a code, consider **Reed-Solomon (RS)** codes $C : \F^k \rightarrow \F^n$, where $\Sigma = \F$ is a finite field[^rs1]:
\begin{align}
C(\m) = (f(1), f(2),\ldots,f(n)),\ \text{where}\ f(X) = \sum_{i=0}^{k-1} m_i X^i
\end{align}
We will likely study RS codes more carefully later but, for now, **the key idea** behind them is that a message in $\F^k$ can be viewed as a degree $k-1$ polynomial $f(X)$.
As a result, the message can be encoded in a redundant fashion by evaluating the $f(X)$ polynomial at $n>k$ (distinct) points.

**Clearly:** Using [Lagrange interpolation](/lagrange-interpolation), we can recover the polynomial $f(X)$ from any set of $\ge k$ evaluations (i.e., any set of $\ge k$ symbols in the codeword). 
So, in this sense, Reed-Solomon codes can recover messages in the presence of lost codeword symbols, a.k.a. **erasures**.

**Not so obvious:** Reed-Solomon can recover messages even if codeword symbols are altered or corrupted rather fully missing.
In this sense, a corrupted symbol in the size-$n$ codeword is referred to as an **error**.
We will explain later how the [rate](#rate) and the [distace](#distance) of a code dictates how many such errors can be corrected.

{: .smallnote}
Beautifully, RS admits an algorithm[^bw]$^,$[^Gao02] that can recover the original $f(X)$ as long as there are less than $\le \floor{\frac{n-k}{2}}$ errors in the size-$n$ codeword.
(We'll see why this is the best we can hope for when we talk about the _singleton bound_.)

### Rate

The **rate** of a code $C : \alphabet^k \rightarrow \alphabet^n$ is:
\begin{align}
\rho\bydef k/n \in (0,1)
\end{align}


{: .note}
We want the rate to be as close as possible to 1, since this minimizes codeword size, keeping the overhead of encoding the message low while guaranteeing recoverability of it in the presence of errors!  
\
High rate: **good**.  
Low rate: _bad_!

### Weight

The **(Hamming) weight** of a codeword $\a$ is the # of positions where $\a$ is non-zero:
\begin{align}
\wt{\a}\bydef \left\|\\{i\in[n] : a_i \ne 0\\}\right\| 
\end{align}

### Distance

The **distance** $\dist{\a,\b}$ between two codewords $\a$ and $\b$ is the # of positions $i$ where $a_i\ne b_i$:
\begin{align}
\dist{\a,\b}\bydef \left|\\{i\in[n] : a_i \ne v_i \\}\right|
\end{align}

The **minimum distance** of a code, often called **distance of $C$**, is:
\begin{align}
\mindist{C} \bydef \min_{\substack{\a,\b\in C\\\a\ne\b}}{\left(\dist{\a,\b}\right)}
\end{align}

For linear codes $C$, one can show this is the same as:
\begin{align}
\mindist{C} \bydef \min_{\substack{\a \in C\\\\\a\ne 0}}{\wt{\a}}
\end{align}

{: .note}
We want the distance of $C$ to be as high as possible, since this allows for correcting more errors.
\
Large distance: **good**.  
Small distance: _bad_!

### Relative distance

{: .note}
It is often more convenient to work with "relative" notions of distance: just divide them by the block length $n$.

The **relative distance** between two codewards $\a$ and $\b$ is their Hamming distance divided by the _block length_ $n$:
\begin{align}
\reldist{\a,\b} \bydef \dist{\a,\b} / n
\end{align}

The **relative minimum distance** of a code $C$, often called **relative distance of $C$**, is:
\begin{align}
\relmindist{C} \bydef \min_{\substack{\a,\b\in C\\\a\ne\b}}{\left(\reldist{\a,\b}\right)}
\end{align}

For linear codes $C$, one can show this is the same as:
\begin{align}
\relmindist{C} \bydef \frac{\min_{\substack{\a \in C\\\\\a\ne 0}}{\wt{\a}}}{n} = \mindist{C} / n
\end{align}

## Glossary

### Basics

 - [x] code
 - [x] message
 - [x] codeword
 - [x] symbols
 - [x] [code] alphabet
 - [x] message alphabet
 - [x] rate
 - [] distance
     - [x] (relative) Hamming distance between two codewords
     - [ ] distance $\delta$ of a codeword from the code
        + [ ] $f$ is $\delta$-far from $\RS[\F,\L,m]$
     - [ ] distance of a code
 - [ ] linear code
     - [ ] generator matrix
     - [ ] Reed-Solomon (RS)
         + [ ] $\RS[\F,\L, m]$
     - [ ] Reed-Muller
 - [ ] singleton bound: min. dist. of linear block code $C$ is $< n - k - 1$ ($\Rightarrow$ can only correct up to $\floor{(n-k)/2}$ errors

### Advanced

 - [ ] subcode (e.g., $\CRS$ vs $\RS$)
 - [ ] unique decoding regime
 - [ ] list decoding
 - [ ] capacity bound
 - [ ] Johnson bound
 - [ ] proximity
 - [ ] $(\delta,\varepsilon)$-correlated agreement for $\RS[\F,\L, m]$
 - [ ] $(\delta, \varepsilon)$ mutual correlated agreement for $\RS[\F,\L, m]$
 - [ ] constrained Reed Solomon codes $\CRS[\F,\L,m, \hat{w}, \sigma]$
 - [ ] tensor codes

<!-- ## Links

 - ?
-->

[^rs1]: Reed-Solomon works over all finite fields (a.k.a., any $\F_q$ where $q$ is a product of prime powers).
[^bw]: e.g., [Berlekampâ€“Welch](https://en.wikipedia.org/wiki/Berlekamp%E2%80%93Welch_algorithm), or [Sudan-Guruswami](https://en.wikipedia.org/wiki/Berlekamp%E2%80%93Welch_algorithm) for list-decoding beyond the singleton bound

## References

For cited works, see below ðŸ‘‡ðŸ‘‡

{% include refs.md %}
